# Importance of subupdates to final logits

The transformer blocks each add a contribution to the residual stream. How important are the contributions to of each block, and more specifically how important are self-attention subupdates and MLP subupdates? There are different ways to answer this question, but the most simple way is to distinguish between two options: Either the subupdates matter significantly to the final logits, or the subupdates do not matter in themselves, instead the output matters because it has effect on the representations used by downstream layers.

The Pythia models use layer norm before applying unembedding layer which makes
it more difficult to answer this problem: If there had been no layer normalization the final representation can be represented as a sum of subupdates and since the dembedding operation is linear it can be applied to each subupdate separately and the final logits can be viewed as sums of logits from each subupdate deembedding.

One simple way of looking at the effect of subupdate versus downstream use of representations is to subtract the subupdate term from the final representations before the layer norm. This gives an ablated set of logits. One can then measure the KL divergence or some other measure of the effects of this. **TODO: Apply this for multiple different sequences and multiple layers ablated.** The problem with this approach is that the KL divergence is not easily interpretable. Alternatively one can view each ablation as a new network and measure the increase in perplexity.

Another aspect is the norms of the subupdates. While the connection between the norms of the subupdates and their effect on the logits are not straightforward, it might be of importance. Here we find that the initial embeddings have low norm (0.64) and that both attention and MLP subupdates have increasing sizes of norms across 2 orders of magnitud. **TODO: Get concrete plots here and specify exactly which data you use**

## Structure in MLP activations
Consider the MLP of a certain layer. If we take a large set of $N$ sequences and restrict ourselves to a single position (random or otherwise chosen) we can retrieve $N$ hidden activation vectors stored as a $H \times d_{M}$ matrix. (here we use $d_{M} = 4d$ to mean the number of neurons in the hidden layer of the MLP which for most Transformers are 4 times the dimensionality of the residual stream) If we do not believe that the each hidden dimension (corresponding to a column of the MLP out-matrix) correspond to an interpretable concept, it might be possible to understand the relationship by making using data in this way.

### Superposition in transformer MLPs
The Anthropic articles suggest that networks represent more features than there are neurons, and applying this to the case of transformer MLPs this would suggest that one should look for a much larger amount of features (I will call them virtual neurons) in an MLP than just $d_{M}$. Regarding this, I first have difficulties with conceptualizing these ideas:

* Does it matter that transformer MLPs are diamond-shaped? The Anthropic experiments use autoencoder like setups, but this is also because in these experiments, the dimensionality of the input correspond to the number of features, which are sparse. How does it translate to the transformer case: Perhaps one has to think of the current residual stream representation $x$ for a as containing information about a number of features much larger than $d$ because of superposition. Then $x$ enters the MLP and there is a large number ($>>d_M$) of virtual feature detectors that do not align with the hidden neurons of the MLP. These are activated to various degrees and this information about the virtual activations are compressed in a $d$ dimensional representation that is written to the stream.
* How does one deal with the fact that we do not just have a representation of a single token but a sequence of token representations? If one was pessimistic, one could worry that the information has diffused across the token representations so that the MLP activations for a single token position will be hard to understand without the others. Because of the setup of the network where the final representations for each token are deembedded to separate distributions over vocabulary, one could be optimistic that the MLP activations are not distributed too much across, the most optimistic version stating that the MLP activations for a single token position contains all the information about the features for the sequence up to that position. Maybe there are people who have good arguments or insight that can answer or comment on this. Otherwise the question becomes whether there are good approaches to investigating this empirically?
* Assuming some loose version of the superposition perspective, the main question of course becomes how one would find these. Are there conceptual aspects that can make it easier to think about: For example, if we use an approach that investigates a matrix $H$ like described above, do we need a really large amount of sequences for some reason? What I mean is, is there some kind of "underdetermination" if one has too few sequences.
* We seem to be able to use dembedding approaches to understand some of the MLP column vectors by looking at top-k. Has there already been, or in general, is there a way to verify the causal effect of this. For example, if we believe that a specific column is related to ice-hockey, can we collect sequences and see that this neuron fires significantly more and then somehow see that if we mean-ablate this neuron, something fails. I am unsure about how hard this is, it seems to be difficult to nail down a way to quantify the effects.
* I have some thoughts about why top-k approaches could be problematic: Since the deembedding vectors are strictly focused on next-token predictions, there might be a pattern in the kind of cases one will or can find where the top-k seems interpretable: In general, all the top-k tokens should be interchangable as a next token, meaning that it should be less likely to see a combination of verbs and nouns among top-k. In general, if there is indeed a neuron or virtual neuron that encodes some quite abstract feature of the sequence, let's say it is the feature that the sequence has a sarcastic tone, it might not really be visible using top-k approaches: A sequence having a sarcastic tone should not influence the the next token prediction in a way that is directly visible by words which are more likely. Maybe I am very wrong here. A related technical question here is how large the space of vocabulary distributions or vocabulary rankings can be produced using the residual stream space. If we have a model with residual stream dimensionality $d$ is it easy to form any combination of top-k chosen from the vocabulary you would like. This should be one of the easier things to investigate. I don't have an intuiton for what the answer is. If the answer is that possible top-k's are limited, this also limits the deembedding top-k approach in general.
* I see that GPT-4 can answer things like "which of these two questions are more likely to be sarcastic" and give explanations. Understanding the mechanisms for answering such a question seems fruitful, the main thing is just whether there is any way to do this at scale.

## Generation based interpretability

* There could be many fruitful approaches to understanding model internals that make use of the generative capacities (sentence completion) or in a more limited form just logits. For large models, we can ask it to generate 10 examples of sentences that are sarcastic and 10 there are not. We can then prompt it to classify its own examples and look at the logits for the answer.
* Are all the features that we attribute to sentences things that are easily describable in language? Even if not, we at least want to find all the feature detectors that detect features that can be described in simple language using a short description. This is already a very ambitious goal. The interesting thing is of course that when we say that we are considering features that are explainable in language this couples well with the fact that these models are indeed models of language. There is no reason not to make use of this connection. One might however also believe that there are limitations to this, especially if we wish to autogenerate interpretations, because in the case where we use the model to interpret itself instead of using a different model, its biases and limitations might easily be included in our model.
* Let us imagine that there is a very very large number of virtual neurons acting as feature detectors hidden in a model. What is then the best case scenario for interpretability? Would it be something like a database where we can have a list of all the feature detectors along with a description of its location within the network, and if distributed how it is distributed, and a long verbal description of exactly what it detects and what its upstream dependencies are and what its downstream uses are? It would be a giant graph of the dependencies between the feature detectors and detailed descriptions of their function, maybe even the ability to have chatbot functionality where the human can ask questions about the details of the virtual neuron.
* I think the idea held by many is that the set of features that are necessary to detect for making good next-token predictions is very similar to the set of features that are necessarsy for general understanding of text data. I believe this enough that I cannot see that this should be a central thing to investigate.
* The idea described above views a major goal of interpreting a model as a process that takes a model and a data set and produces an advanced "index of features", a process that could be called just indexing the features.
  
# Causality

* If we claim to know that a virtual neuron exists for a feature then we want to also have a causal understanding of this. We want to be able to knock out this neuron from the model, resulting in a model that is generally identical in behavior in all the cases where detecting this feature is not relevant, meaning that it should have have roughly the same total perplexity on the validation set. However, it should not be able to solve tasks or token-predictions that involve making use of the detection of these features in the input sequence.
* This conception of what we require of a causal interpretation of a virtual neuron can maybe be used as a starting point for the methods we want to develop. One idea is the following: Take a set of sequences from the data set and for each sequence, use the model itself to enumerate a list of features that a model might detect in that sequence. This could give you a non-exhaustive list of features that might be worth indexing. For each of these features we might now want to use the model to help us understand where the feature detectors are located within the model.
* One option is to ask the model to generate a large amount of example sequences that contain the feature along with negative examples that contain the feature to a smaller degree. We then have at least two options: First, we can try to understand the feature by making use of the causal structure of the generation: By this I mean that we try to investigate the activations, logits, gradients  etc. that are produced in producing these examples based on a prompt that asks for examples of sequences that contain this feature. Secondly, we can try to understand the feature by presenting these examples, for example in pairs of postive and negative examples and ask the model to identify which of the two sequences contain the feature and understand the activations, logits, gradients etc. that are involved in making this classification. Ideally there should be a good overlap between these two methods.
* If we have methods for identifying virtual neurons from the generative and classification approaches, we next want to have various ways of knocking out the virtual neuron by ablation. The effect of this ablation should be that the performs much much worse on both the generative and classification tasks on a validation set produced by the model. Methods for identifying and knocking out the virtual neuron might make use of a training set.
* My first idea for how to identify and knock out a virtual neuron is the following sketch: Take a generated set of postive and negative pairs of sequences for a proposed feature, for example the presence of sarcasm. Call the sequences A and B. Prompt the model to say "The sequence which is most likely to have the presence of the feature of sarcasm is sentence ..." and then look at the logits of A and B. If the logit is higher for the sentence which actually (labelled by the model itself) contains sarcasm then the classifier works. Let us say that our method looks for feature detectors that belong to MLPs, and we hypothesize that the detector lives in a single MLP at least dominantly. A (non-virtual) neuron in an MLP fires when the input vector has a high dot product with the associated neuron row-vector (from the in-matrix), and the column vector then describes how this feature is represented in the residual stream. The row vector is therefore the detector: The row vector for detecting a feature is placed so that an input vector which encodes multiple simple features that are necessary for detecting a more complex feature will have a high dot product with it. Only to the degree this dot product is high enough will we write a new vector to the residual stream. The written vector does not need to have a high cosine similarity to the row vector, and therefore it allows the neuron to make a real association: If we see these two simple features it reminds me of this completely third more complex thing, which will be used upstream in later layers because it itself has a high dot product with other concepts:
* Empirically, looking at Pythia models: How much do row and column vectors align, take a specific neuron: Does its column vector have highest cosine similarity to its own row vector or not? We find that for 410m and 1b models, looking at cosine similarities, the trend is that they have a slightly negative cosine similarity with each other while the cosine similarity with other vectors are centered around 0 in the interval (-0.1,0.1). This is consistent across layers. I would have hypothesized that they were generally had similarity 0 but would not have guessed that they were negative.
* I don't know if others find this interesting, but I did not expect to find this pattern: Consider a neuron in an MLP in some layer of a GPT model. It has an associated row vector in the incoming affine transformation and an associated column vector in the outgoing affine transformation. The finding is just that the trend is that the cosine similarities between them is negative (large variance approx 0.2), while cosine similarities between other combinations of rows and column vectors are are centered around 0. I have checked multiple layers in Pythia 410m and 1b. I will clean up the notebook later I have used later, but here it is https://colab.research.google.com/drive/11kq2__1lbh3CaL3raLdqcwNi3uMM-Ij2?usp=sharing in case you are interested in having a look. Another pattern is that the cosine similarity seems to be lowest (approx -0.4) in the middle of the network, giving a V-shaped curve of median of cosine similarities. I don't know what implications this has for thinking about superposition and polysemanticity (https://transformer-circuits.pub/) in transformer MLPs. 