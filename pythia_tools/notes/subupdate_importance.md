# Importance of subupdates to final logits

The transformer blocks each add a contribution to the residual stream. How important are the contributions to of each block, and more specifically how important are self-attention subupdates and MLP subupdates? There are different ways to answer this question, but the most simple way is to distinguish between two options: Either the subupdates matter significantly to the final logits, or the subupdates do not matter in themselves, instead the output matters because it has effect on the representations used by downstream layers.

The Pythia models use layer norm before applying unembedding layer which makes
it more difficult to answer this problem: If there had been no layer normalization the final representation can be represented as a sum of subupdates and since the dembedding operation is linear it can be applied to each subupdate separately and the final logits can be viewed as sums of logits from each subupdate deembedding.

One simple way of looking at the effect of subupdate versus downstream use of representations is to subtract the subupdate term from the final representations before the layer norm. This gives an ablated set of logits. One can then measure the KL divergence or some other measure of the effects of this. **TODO: Apply this for multiple different sequences and multiple layers ablated.** The problem with this approach is that the KL divergence is not easily interpretable. Alternatively one can view each ablation as a new network and measure the increase in perplexity.

Another aspect is the norms of the subupdates. While the connection between the norms of the subupdates and their effect on the logits are not straightforward, it might be of importance. Here we find that the initial embeddings have low norm (0.64) and that both attention and MLP subupdates have increasing sizes of norms across 2 orders of magnitud. **TODO: Get concrete plots here and specify exactly which data you use**

## Structure in MLP activations
Consider the MLP of a certain layer. If we take a large set of $N$ sequences and restrict ourselves to a single position (random or otherwise chosen) we can retrieve $N$ hidden activation vectors stored as a $H \times d_{M}$ matrix. (here we use $d_{M} = 4d$ to mean the number of neurons in the hidden layer of the MLP which for most Transformers are 4 times the dimensionality of the residual stream) If we do not believe that the each hidden dimension (corresponding to a column of the MLP out-matrix) correspond to an interpretable concept, it might be possible to understand the relationship by making using data in this way.

### Superposition in transformer MLPs
The Anthropic articles suggest that networks represent more features than there are neurons, and applying this to the case of transformer MLPs this would suggest that one should look for a much larger amount of features (I will call them virtual neurons) in an MLP than just $d_{M}$. Regarding this, I first have difficulties with conceptualizing these ideas:

* Does it matter that transformer MLPs are diamond-shaped? The Anthropic experiments use autoencoder like setups, but this is also because in these experiments, the dimensionality of the input correspond to the number of features, which are sparse. How does it translate to the transformer case: Perhaps one has to think of the current residual stream representation $x$ for a as containing information about a number of features much larger than $d$ because of superposition. Then $x$ enters the MLP and there is a large number ($>>d_M$) of virtual feature detectors that do not align with the hidden neurons of the MLP. These are activated to various degrees and this information about the virtual activations are compressed in a $d$ dimensional representation that is written to the stream.
* How does one deal with the fact that we do not just have a representation of a single token but a sequence of token representations? If one was pessimistic, one could worry that the information has diffused across the token representations so that the MLP activations for a single token position will be hard to understand without the others. Because of the setup of the network where the final representations for each token are deembedded to separate distributions over vocabulary, one could be optimistic that the MLP activations are not distributed too much across, the most optimistic version stating that the MLP activations for a single token position contains all the information about the features for the sequence up to that position. Maybe there are people who have good arguments or insight that can answer or comment on this. Otherwise the question becomes whether there are good approaches to investigating this empirically?
* Assuming some loose version of the superposition perspective, the main question of course becomes how one would find these. Are there conceptual aspects that can make it easier to think about: For example, if we use an approach that investigates a matrix $H$ like described above, do we need a really large amount of sequences for some reason? What I mean is, is there some kind of "underdetermination" if one has too few sequences.
* We seem to be able to use dembedding approaches to understand some of the MLP column vectors by looking at top-k. Has there already been, or in general, is there a way to verify the causal effect of this. For example, if we believe that a specific column is related to ice-hockey, can we collect sequences and see that this neuron fires significantly more and then somehow see that if we mean-ablate this neuron, something fails. I am unsure about how hard this is, it seems to be difficult to nail down a way to quantify the effects.
* I have some thoughts about why top-k approaches could be problematic: Since the deembedding vectors are strictly focused on next-token predictions, there might be a pattern in the kind of cases one will or can find where the top-k seems interpretable: In general, all the top-k tokens should be interchangable as a next token, meaning that it should be less likely to see a combination of verbs and nouns among top-k. In general, if there is indeed a neuron or virtual neuron that encodes some quite abstract feature of the sequence, let's say it is the feature that the sequence has a sarcastic tone, it might not really be visible using top-k approaches: A sequence having a sarcastic tone should not influence the the next token prediction in a way that is directly visible by words which are more likely. Maybe I am very wrong here. A related technical question here is how large the space of vocabulary distributions or vocabulary rankings can be produced using the residual stream space. If we have a model with residual stream dimensionality $d$ is it easy to form any combination of top-k chosen from the vocabulary you would like. This should be one of the easier things to investigate. I don't have an intuiton for what the answer is. If the answer is that possible top-k's are limited, this also limits the deembedding top-k approach in general.
* I see that GPT-4 can answer things like "which of these two questions are more likely to be sarcastic" and give explanations. Understanding the mechanisms for answering such a question seems fruitful, the main thing is just whether there is any way to do this at scale.
 